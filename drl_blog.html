<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<meta http-equiv="x-ua-compatible" content="ie=edge">
<meta name="viewport" content="width=device-width">
<script src="https://code.jquery.com/jquery-3.5.1.slim.min.js" integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj" crossorigin="anonymous"></script>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.5.3/dist/css/bootstrap.min.css" integrity="sha384-TX8t27EcRE3e/ihU7zmQxVncDAy5uIKz4rEkgIXeMed4M0jlfIDPvg6uqKI2xXr2" crossorigin="anonymous">
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/10.0.3/styles/default.min.css">
<link href="css/custom.css" rel="stylesheet"/>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script>
MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']]
  },
  svg: {
    fontCache: 'global'
  },
  tex: {
    tags: 'all'
  }
};
</script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"></script>
</head>

<body>
<div class="jumbotron">
  <div class="container">
    <h1 class="text-light">Deep Reinforcement Learning</h1>
    <h3 class="text-light">from scratch</h3>
  </div>
</div>

<div class="container">
<div class="mb-2">
<p>
This long article will explain to you what Deep Reinforment Learning is all about and we will teach you the
theory and implementation from scratch. 
This article summarizes lecture <a href="http://rail.eecs.berkeley.edu/deeprlcourse/">CS285</a> from UCLA Berkeley,
and I strongly recommend you to watch the lecture for more in depth explanations. 
</p>
</div>
   
<div class="mb-5"> 
<h2>The Goal of Reinforcement Learning</h2>
<p>
Essentially Reinforcement Learning is a special class of Probability Graphical Model with the joint probability:
\begin{align}
p_{\theta}(s_1, a_1, \ldots, s_T, a_T) = p(s_1) \prod_{t=1}^{T} \pi_{\theta}(a_t \vert s_t) p(s_{t+1} \vert s_t, a_t)
\end{align}
where \(\pi_{\theta}(a_t \vert s_t)\) is called a policy, which is just conditional probability 
of action \(a_t\) given a state \(s_t\) at time \(t\).
The transition probability from \(s_t, a_t\) to \(s_{t+1}\) is given by \(p(s_{t+1} \vert s_t, a_t)\). 
I am just going to spoil the game for you by telling you that 99% of the time we do not know both
the policy and transition probability.  
The goal of the game is to maximize the sum of the rewards:
\begin{align}
\theta^* &= \mathop{\text{arg max}}_{\theta} J(\theta)
=\mathop{\text{arg max}}_{\theta} \mathop{\mathbb{E}}_{\tau \sim p_{\theta}(\tau)} \left[ \sum_{t}
\gamma^t  r(s_t, a_t) \right],
\end{align}
with the term \(\gamma\) as discount factor.
</p>
<p>
Your goal is to maximize the total reward by interacting with the environment.
You are free to do whatever you want, for instance learning a policy, estimating transition probability or
expectation value of rewards.
Let us try to directly optimize the objective above, and see how far can we go. 
</p>
</div>

<div class="mb-5">
<h2>Policy Gradient</h2>
<p>
So the first thing a physicist would do when he or she encountered the expectation term is to convert it
to sample mean:
\begin{align}
J(\theta) &= \mathop{\mathbb{E}}_{\tau \sim p_{\theta}(\tau)} \left[ \sum_{t} \gamma^t r(s_t, a_t) \right]
\approx \frac{1}{N} \sum_{i=1}^{N} \sum_{t=1}^{T} \gamma^t r(s_{i,t}, a_{i,t})
\end{align}
Notice that when you sample your trajectory to get the rewards, you need to follow your current policy. 
Let us denote the sum of all rewards as \(r(\tau)\). 
Let us switch back to formal definition of expectation and apply gradient operator to \(J(\theta)\):
\begin{align}
\nabla_{\theta} J(\theta) &= \int \nabla_{\theta} p_{\theta}(\tau) r(\tau) d\tau \\
&= \int p_{\theta} \nabla_{\theta} \log p_{\theta}(\tau) r(\tau) d\tau \\
&= \mathop{\mathbb{E}}_{\tau \sim p_{\theta}(\tau)} \left[ \nabla_{\theta} \log p_{\theta}(\tau) r(\tau)\right] \\
&= \mathop{\mathbb{E}}_{\tau \sim p_{\theta}(\tau)} \left[ \left( \sum_{t=1}^{T} \nabla_{\theta}
\log \pi_{\theta}(a_t \vert s_t) \right) \left( \sum_{t=1}^{T} \gamma^t r(s_t, a_t) \right) \right] 
\label{eq:simplify} \\
&\approx \frac{1}{N}\sum_{i=1}^{N} \left( \sum_{t=1}^{T} \nabla_{\theta}
\log \pi_{\theta}(a_{i,t} \vert s_{i,t}) \right) \left( \sum_{t=1}^{T} \gamma^t r(s_{i,t}, a_{i,t}) \right).\\ 
\label{eq:grad}
&\equiv \frac{1}{N}\sum_{i=1}^{N} \sum_{t=1}^{T} \nabla_{\theta}
\log \pi_{\theta}(a_{i,t} \vert s_{i,t}) \Psi_t. 
\end{align}
In the second step we have used the identity \(\nabla \log A = \nabla A / A \).
Note that as only the policy depends on \(\theta\), only its derivative survives inside the \(p_{\theta}\) 
term, hence the simplification in eq.\eqref{eq:simplify}. 
We replace the sample mean back in the expectation term in eq.\eqref{eq:grad} .
Once we have the gradient term, we can update the parameter \(\theta\). 
Notice that we have replaced the sum of rewards with a more generalized function \(\Psi_t\),
which will become convenient later. 
We obtain our first Reinforcement Learning algorithm, the REINFORCE algorithm, which can be summarized 
by the following steps:
</p>
<div class="card bg-light mb-3">
<div class="card-body">
<div class="card-header">REINFORCE algorithm</div>
  <ol>
    <li>Sample the trajectory from a fixed policy.</li>
    <li>Obtain \(\nabla_{\theta} J(\theta)\) from the equation \eqref{eq:grad}.</li>
    <li>\( \theta \leftarrow \theta + \alpha  \nabla_{\theta} J(\theta) \)</li>
  </ol>
</div>
</div>
<p>
We have our first example of an on-policy algorithm. 
On-policy means that we have to stick to our current policy when we sample a trajectory with fixed \(\theta\).
Old trajectories from previous policy, i.e. different \(\theta\) should be discarded. 
We can see from step 1 of REINFORCE that this step is unavoidable. 
</p>

<p>
I should stress that if you compare the terms in eq.\eqref{eq:grad} with loss function in supervised learning,
the \(\nabla_{\theta} \log \pi_{\theta} \) term dictates the changes in policy, while the reward term controls
the magnitude of the backpropagation. 
You can think of it as a variable learning rate, and you prefer to correct the parameter  \( \theta \) 
to collect more rewards. 
As mentioned before, policy is just a probability distribution.
We can take a simple Gaussian distribution defined as 
\( \pi_{\theta}(a_t \vert s_t) = \mathcal{N}(f(s_t);\Sigma) \) where \(f \) is typically a neural network.
</p>
</div>

<div class="mb-5">
<h4>Problem with Variance</h4>
<p>
Sadly, REINFORCE does not work most of the time. 
The problem lies in the reward term in eq.\eqref{eq:grad}.
In supervised learning we do not have that extra term as the gradient dictates the change of parameter that
we should undertake to improve our model. 
In RL the convergence of the vanilla policy gradient defined above depends sensitively on \(r(\tau) \).
Unfortunately different use cases have different rewards.
One extreme example would be to have a situation where good examples have zero reward. 
The system will have hard time learning from good samples. 
What we need to do is to normalize the sum of reward so that the good example will have positive reward,
while the bad example should have negative reward, i.e. 
\begin{align}
\Psi = \sum_{t=1}^T \gamma^t r(s_{i,t}, a_{i,t}) - \text{something}.
\end{align}
We can take a constant \(b\) as a simple "something" for baseline comparison. 
Why we are allowed to subtract something in eq.\eqref{eq:grad}? 
One can show that the gradient of the sum or integral of probability density function vanishes as
the integral \(\int p_{\theta}(\tau) d\tau = 1\).
Hence we are basically subtracting a zero in eq.\eqref{eq:grad}.
We can further simplify the sum by noting that policy at future time should not affect the reward at time \(t\).
This is just a simple causality analysis and has nothing to do with Markov process.
Let us update eq.\eqref{eq:grad}:
\begin{align}
\nabla_{\theta} J(\theta) &\approx \frac{1}{N}\sum_{i=1}^{N} \left( \sum_{t=1}^{T} \nabla_{\theta}
\log \pi_{\theta}(a_{i,t} \vert s_{i,t}) \right) \left( \sum_{t'=t}^{T} \gamma^{t'-t} r(s_{i,t'}, a_{i,t'}) 
-b\right). \label{eq:grad2}
\end{align}
This term \( \sum_{t'=t}^{T} \gamma^{t'-t} r(s_{i,t'}, a_{i,t'})-b \) is unbiased, but has high variance
single-sample estimate. 
Note that the discount factor only discount the reward, but not the gradient.
In some sense we are not doing the true discount as future action should matter less than current action,
and we should really discount the term \(\pi_{\theta}(a_t \vert s_t)\).
However in practice we want our agent to run forever and hence all action should be equally important. 
Therefore only the reward is discounted.
</p>
</div>

<div class="mb-5">
<h2>Actor-Critic</h2>
<p>
The reward term in eq.\eqref{eq:grad2} is based on single sample trajectory, which means that the variance is 
still large.
It would be more desirable to average out the rewards from more sample trajectories.
Ideally we want to replace the term:
\begin{align}
\sum_{t'=t}^T \gamma^{t'-t}r(s_{t'}, a_{t'}) \rightarrow 
\sum_{t'=t}^T \mathbb{E}_{\pi_{\theta}} \left[ \gamma^{t'-t}r(s_{t'}, a_{t'}) \vert s_{t}, a_{t} \right]
\equiv Q^{\pi}(s_t, a_t), \label{eq:saf}
\end{align}
with the expected reward-to-go, also known as <i>State-action value</i> or Q-function.
It represents the total reward obtained by taking action \(a_t\) in state \(s_t\).
Notice the supercript \(\pi \) for the State-action value function, as \(Q(s_t, a_t)\) has a fixed policy.
If we accept such a proposal, then we should also improve our baseline in eq.\eqref{eq:grad2}.
As mentioned before that the baseline serves like an average of the reward, perhaps we can just average all 
the actions that can be taken in \(Q^{\pi}(s_t, a_t) \), defined as the 
<i>State Value</i> function or just <i>Value</i> function:
\begin{align}
V^{\pi}(s_t) \equiv \mathop{\mathbb{E}}_{a_t \sim \pi_{\theta}(a_t \vert s_t)} \left[Q^{\pi}(s_t, a_t) \right].
\label{eq:value}
\end{align}
The Value function represents the total reward from state \(s_t\) and can be used as baseline.
The difference between State-action value and the Value function is called Advantage function:
\begin{align}
A^{\pi}(s_t, a_t) = Q^{\pi}(s_t, a_t) - V^{\pi}(s_t).
\end{align}
I should stress that the Value function is independent of action, as we have averaged out all actions for a 
given state.
To my fellow physicist friends, I notice that the Advantage function has a form of Lagrangian
in classical mechanics, with the Q-function acting as kinetic energy term while the Value function
acting as the potential term. 
Would be great if we can take a look at the analogy closely next time.
Our new gradient function has now the following form:
\begin{align}
\nabla_{\theta} J(\theta) &\approx \frac{1}{N}\sum_{i=1}^{N} \sum_{t=1}^{T} \nabla_{\theta}
\log \pi_{\theta}(a_{i,t} \vert s_{i,t}) A^{\pi}(s_{i,t}, a_{i,t}). \label{eq:grad3}
\end{align}
The Advantage function is any function that you can introduce to lower the variance, 
if it can estimate the reward better. 
It will give crappy results if it is not good in predicting the reward.
Notice however that as we have introduced an approximator in our equation, we have also introduced biases
in eq.\eqref{eq:grad3}. 
</p>

<p>
It is easier to fit the Value function as it only depends on the state.
The less argument your function has, the easier it will learn.
Let us manipulate eq.\eqref{eq:saf} to manifest the value function:
\begin{align}
Q^{\pi}(s_t, a_t) &= 
\sum_{t'=t}^T \mathbb{E}_{\pi_{\theta}} \left[ \gamma^{t'-t} r(s_{t'}, a_{t'}) \vert s_{t}, a_{t} 
\right] \label{eq:saf1} \\
&= r(s_t, a_t) + 
\sum_{t'=t+1}^T \mathbb{E}_{\pi_{\theta}} \left[ \gamma^{t'-t} r(s_{t'}, a_{t'}) \vert s_{t}, a_{t} \right] 
\label{eq:saf2}\\
&= r(s_t, a_t) +
\gamma \mathop{\mathbb{E}}_{s_{t+1} \sim p(s_{t+1} \vert s_t, a_t)} \left[ V^{\pi}(s_{t+1}) \right] 
\label{eq:saf3}\\
&\approx r(s_t, a_t) + \gamma V^{\pi}(s_{t+1}). \label{eq:saf4}
\end{align}
Because \(s_t,a_t\) is known at time \(t\), we can take out the reward at time \(t\) out of the expectation,
hence eq.\eqref{eq:saf2}.
By using eq.\eqref{eq:value}, we can see that eq.\eqref{eq:saf2} can be written as eq.\eqref{eq:saf3},
which is still exact. 
Now we approximate that for state \(s_{t+1}\), we take only 1 sample instead of averaging the Value function
for all state at \(t+1\), which is in eq.\eqref{eq:saf4}.
Note that we still take the expectation for state on \(t+2\) onwards. 
The reason for this simplification is to have the Advantage function depending only on the Value function:
\begin{align}
A^{\pi}(s_t, a_t) \approx r(s_t,a_t) +\gamma V^{\pi}(s_{t+1}) - V^{\pi}(s_t). \label{eq:adv}
\end{align}
Now we can just fit the Value function. 
</p>
</div>

<div class="mb-5">
<h4>Policy Evaluation</h4>
<p>
Ideally at a fixed state \(s_t\), we want to sum all the rewards accumulated from that state onwards with multiple
sample trajectories. 
However, most of the time you cannot roll back your environment to the exact state \(s_t\) to generate 
another sample trajectory, unless you are using a simulator. 
That is why we will use a function approximator \(\hat{V}^{\pi}_{\phi}(s_i) \) like neural network
with parameter \(\phi\) as Value function.
The assumption we have made here is that the neural network will generate trajectories that are close 
to the fixed \(s_t\).
Naively we generate the training data pair \({(s_{i,t}, \sum_{t'=t}^T \gamma^{t'-t}r(s_{i,t'}, a_{i,t'}))} \) 
and minimize the error:
\begin{align}
\mathcal{L}(\phi) = \frac{1}{2} \sum_i \left\lVert \hat{V}^{\pi}_{\phi}(s_i) - \sum_{t'=t}^T \gamma^{t'-t}
r(s_{i,t'}, a_{i,t'}) \right\rVert^2. \label{eq:loss1}
\end{align} 
Instead of using full trajectories, we can bootstrap the approximator by plugging it back during sampling of 
training data. 
We would have the training set \({(s_{i,t}, r(s_{i,t}, a_{i,t})+ \gamma\hat{V}^{\pi}_{\phi}(s_{i, t+1}))} \) 
to minimize 
\begin{align}
\mathcal{L}(\phi) = \frac{1}{2} \sum_i \left\lVert \hat{V}^{\pi}_{\phi}(s_i) - 
r(s_{i,t'}, a_{i,t'}) - \gamma \hat{V}^{\pi}_{\phi'}(s_{i, t+1}) \right\rVert^2. \label{eq:loss2}
\end{align} 
The parameter \(\phi'\) represents the value from previous fit.
Now we can use the function approximator \(\hat{V}^{\pi}_{\phi}(s_i) \) to sample summed reward.
The batch <i>Actor-Critic</i> algorithm is defined as below:
</p>
<div class="card">
<div class="card-body bg-light mb-3">
<div class="card-header">Batch Actor-Critic Algorithm</div>
  <ol>
    <li>Sample the trajectory from a fixed policy \(\pi_{\theta}(a \vert s)\).</li>
    <li>Fit \(\hat{V}^{\pi}_{\phi}(s_i) \) by minimizing eq.\eqref{eq:loss1} or eq.\eqref{eq:loss2}.</li>
    <li>Evaluate the Advantage function eq.\eqref{eq:adv} by replacing the Value function with 
        \(\hat{V}^{\pi}_{\phi}(s_i) \)</li>
    <li>Obtain \(\nabla_{\theta} J(\theta)\) from the equation \eqref{eq:grad3}.</li>
    <li>\( \theta \leftarrow \theta + \alpha  \nabla_{\theta} J(\theta) \).</li>
    <li>Repeat step 1 till 5.</li>
  </ol>
</div>
</div>
<p>
We can also use the online version of this algorithm, where we only take an action 
\(a\sim \pi_{\theta}(a \vert s)\) to get one tuple of \((s,a,s',r)\) and run through the algorithm above
with one sample. 
The convergence is typically poor when we have neural network for the policy and critic. 
This problem can be ameliorated with parallel workers for step 1, and we update the neural network parameters
\(\phi\) and \(\theta\) by averaging the losses from all the sample. 
</p>
</div>

<div class="mb-5">
<h4>Critic as Baseline</h4>
<p>
Let us compare the \(\Psi\) term for Actor-Critic model and policy gradient.
</p>
<div class="row">
 <div class="col-sm-6">
  <div class="card bg-light mb-3">
   <div class="class-body">
    <div class="card-header">Actor-Critic</div>
     \begin{align}
     \Psi_t = r(s_{i,t}, a_{i,t}) + \gamma \hat{V}^{\pi}_{\phi}(s_{i,t+1}) - \hat{V}^{\pi}_{\phi}(s_{i,t})
     \nonumber
     \end{align}
     <ul>
      <li>Lower variance</li>
      <li>Bias if critic is not perfect</li>
     </ul>
   </div>
  </div>
 </div>
 <div class="col-sm-6">
  <div class="card bg-light mb-3">
   <div class="class-body">
    <div class="card-header">Policy Gradient</div>
     \begin{align}
     \Psi_t = \sum_{t'=t}^T \gamma^{t'-t} r(s_{i,t'}, a_{i,t'}) -b
     \nonumber
     \end{align}
     <ul>
      <li>Unbiased</li>
      <li>High variance due to single-sample</li>
     </ul>
   </div>
  </div>
 </div>
</div>
<p>
It would be nice to combine both the advantages from Actor-Critic and policy gradient, to have:
\begin{align}
\Psi_t = \sum_{t'=t}^T \gamma^{t'-t} r(s_{i,t'}, a_{i,t'}) - \hat{V}^{\pi}_{\phi}(s_{i,t}).\label{eq:genrew}
\end{align}
The eq.\eqref{eq:genrew} enables us to lower variance with bias tradeoff. 
Heuristically we know that state and action not far from current state \(s_t\) will be more or less similar,
while the variance is large the further we move to the future. 
Therefore we can use the single-sample estimate up to time \(n>1\) and then use the critic 
to estimate the rest:
\begin{align}
\Psi_t(n) = \sum_{t'=t}^{t+n} \gamma^{t'-t} r(s_{i,t'}, a_{i,t'}) 
+ \gamma^n \hat{V}^{\pi}_{\phi}(s_{i,t+n})
- \hat{V}^{\pi}_{\phi}(s_{i,t}).\label{eq:genrew2}
\end{align}
Why stop here with just 1 fixed n? 
We can sum all \(\Psi_t(n) \) weighted with exponential falloff \(w_n \sim \lambda^{n-1}\) to obtain
the Generalized Advantage Estimation:
\begin{align}
\Psi_t^{\text{GAE}} = \sum_{t'=t}^{\infty}(\gamma \lambda)^{t'-t}
\left( r(s_{i,t'}, a_{i,t'})
+ \gamma \hat{V}^{\pi}_{\phi}(s_{i,t'+1})
- \hat{V}^{\pi}_{\phi}(s_{i,t'}) \right).\label{eq:gengae}
\end{align}
This advantage function massively reduces variance by reducing lambda.
In some sense it shows that the discount factor is also reducing variance, as one can redefine
\(\hat{\gamma} = \gamma \lambda\) to be our new discount factor. 
</p>
</div>

<div class="mb-5">
<h2>Value Function Method</h2>
<p>
From batch Actor-Critic algorithm we notice that we use the Advantage function \(A^{\pi}(s_t,a_t)\)
as the generalized reward \(\Psi_t\) for the policy gradient. 
If we can choose the best action that maximizes \(A^{\pi}(s_t, a_t)\), we could just forget about 
taking gradient, and just update our policy using that action.
Let us define a new policy that is as good as policy \(\pi(a_t \vert s_t)\) with:
\begin{align}
\pi'(a_t \vert s_t) = \begin{cases}
    1, \, a_t = \mathop{\text{arg max}}\limits_{a_t} A^{\pi(s_t, a_t)} \\
    0, \, \text{otherwise.}
    \end{cases}
\end{align}
The idea of evaluating the Advantage function and setting the new policy according to the best action is 
called <i>Policy Iteration</i>.
</p>
<div class="card">
<div class="card-body bg-light mb-3">
<div class="card-header">Policy Iteration</div>
  <ol>
    <li>Evaluate \(A^{\pi}(s,a) = r(s,a) + \gamma \underset{s'}{\mathbb{E}}[V^{\pi}(s')] - V^{\pi}(s)\).</li>
    <li>Set \(\pi \leftarrow \pi'\)</li>
  </ol>
</div>
</div>
<p>
If we assume that we know the transition probability \(p(s' \vert s, a)\) and we have small discrete set
of state and action, we can bootstrap the update of Value function:
\begin{align}
V^{\pi}(s) \leftarrow \mathop{\mathbb{E}}_{a\sim\pi(a \vert s)} \left[ r(s,a) +
\gamma \mathop{\mathbb{E}}_{s'\sim p(s' \vert s,a)} \left[ V^{\pi}(s')\right] \right],
\end{align}
where we just use the current Value function on the r.h.s.of the equation above. 
The process is called <i>Policy Evaluation</i>.
Recall that the State-action value is \(Q^{\pi}(s,a) = r(s,a) + \gamma \mathbb{E}[V^{\pi}(s')] \)
and taking the arg max of Advantage function is the same as taking the arg max of State-action function,
as the Value function is independent of action.  
</p>

<p>
Can we skip the policy directly? As we can see that we just need to update \(V(s)\) and \(Q(s,a)\).
Policy is just an intermediate step. 
Hence instead of taking the arg max of State-action function, we just the the max of \(Q(s,a)\) directly.
We obtain the <i>Value Iteration algorithm</i>:
</p>
<div class="card">
<div class="card-body bg-light mb-3">
<div class="card-header">Value Iteration</div>
  <ol>
    <li>Evaluate \(Q(s,a) \leftarrow r(s,a) + \gamma \underset{s'}{\mathbb{E}}[V^{\pi}(s')] \).</li>
    <li>Set \(V(s) \leftarrow \underset{a}{\max} Q(s,a) \)</li>
  </ol>
</div>
</div>
<p>
Fun fact, physicist often uses fields to describe interation between particles. 
Often, one can skip the field completely and describe the interation of one particle as a function 
of another particle.
This type of model often does not manifest the causality structure and hence tends to produce
"action-at-instance" effect. 
Newtonian gravity theory falls under this class, while Einstein general relativity requires gravitation
field. 
</p>
</div>

<div class="mb-5">
<h4>Fitting Value Function or State-action Function</h4>
<p>
Normally the number of state is gigantic and we cannot calculate \(V(s)\) by brute force.
We need to somehow approximate the Value function, and neural network is a good candidate.
Suppose we have \(V\) as neural network with parameter \(\phi\), we can have <i>Fitted Value Iteration 
algorithm</i> as follows:
</p>
<div class="card">
<div class="card-body bg-light mb-3">
<div class="card-header">Fitted Value Iteration</div>
  <ol>
    <li>Set \(y_i \leftarrow \underset{a}{\max} (r(s_i,a_i) + \gamma \underset{s'_i}{\mathbb{E}}
     [V_{\phi}(s'_i)] ) \).</li>
    <li>Set \(\phi \leftarrow \underset{\phi}{\text{arg max}} \frac{1}{2} \underset{i}{\sum} \lVert
      V_{\phi}(s_i) - y_i \rVert^2 \).</li>
  </ol>
</div>
</div>
<p>
The problem with (Fitted) Value Iteration is that we need to know the transition probability so that
we can sample from it in the expectation.
Also the max function requires you to have multiple action at current state, hence you need to 
keep resetting your agent back to this fixed state to retake another action, unless 
you know the transition probability. 
Perhaps if we have a policy again, we can ameliorate parts of the problem. 
Let us just perform policy evaluation on \(Q^{\pi}(s,a) \) instead of \(V^{\pi}(s) \):
\begin{align}
Q^{\pi}(s,a) \leftarrow r(s,a) + \gamma \underset{s'}{\mathbb{E}}[Q^{\pi}(s', \pi(s')].
\end{align}
Now we can fit the above function using samples that can be obtained from another policy. 
Since we use policy \(\pi\) in \(Q^{\pi}\) on the r.h.s., we can treat the data tuple \((s,a,s')\)
as independent of policy, and reuse them in the expectation calculation.
We do not need to know the transition probabilities, just the sample \((s,a,s')\). 
</p>

<p>
Can we have Fitted Value Iteration method for the State-action function? 
Yes it is possible, when we approximate 
\(\mathbb{E}[V_{\phi}(s'_i)] \approx \max_{a'}Q_{\phi}(s'_i, a'_i) \) in Fitted Value Iteration.
We obtain the Fitted Q Iteration algorithm:
</p>
<div class="card">
<div class="card-body bg-light mb-3">
<div class="card-header">Fitted Q Iteration</div>
  <ol>
    <li>Collect dataset \({(s_i, a_i, s'_i, r_i)}\) using some policy.</li>
    <li>Set \(y_i \leftarrow r(s_i,a_i) + \gamma \underset{a'}{\max} Q_{\phi}(s'_i, a'_i)  \).</li>
    <li>Set \(\phi \leftarrow \underset{\phi}{\text{arg max}} \frac{1}{2} \underset{i}{\sum} \lVert
      Q_{\phi}(s_i, a_i) - y_i \rVert^2 \).</li>
    <li>Repeat step 2 and 3 for \(K\) times.</li>
    <li>Repeat step 1.</li>
  </ol>
</div>
</div>
<p>
The convergence is not guaranteed when you use function approximator like neural network for \(Q\).
Notice that like Fitted Value Iteration, we can use off policy sample, which is not the case in Actor-Critic.
In step 2 the max function is improving the policy, as you can think of \(Q\) as a model that gives 
you different value when you take different action.
Taking the best action means improving the policy.
Step 3 is required to improve the parameters \(\phi\).
If \(K=1\), we would have the online version of the algorithm. 
Note that you may not want to use the greedy algorithm if you are using the online Q Iteration algorithm.
Intuitively your baby robot will learn only to crawl and cannot learn to walk if you apply greedy algorithm,
as it will just take the steps that maximise its reward, which likely to be crawling at the beginning of
the learning phase.
It will never want to explore alternatives that might reap higher rewards.
We can impose a small probability \(\epsilon\) that the agent will explore, and \(1-\epsilon\) for the agent
to follow greedy algorithm. 
We will come to exploration later.
</p>
</div>


<div class="mb-5">
<h2>Q-Learning</h2>
<p>
What could go wrong with our online Fitted Q iteration method above?
First of all, step 3 in online Fitted Q Iteration is not gradient descent, as \(y_i\) contains the term
\(Q_{\phi}(s'_i, a'_i) \), which we did not take into account in gradient descent.
Or in software term, the backprop of parameter \(\phi\) does not flow through \(y_i\).
</p>

<p>
Second problem that we have is that the dataset \((s_i, a_i, s'_i, r_i)\) is generally correlated,
as your next data pair will be very close to your current state. 
However recall that stochastic gradient descent requires i.i.d. dataset!
Your fit will be always optimised to your previous dataset, it can never learn to generalize.
Updating your neural network parameter with 1 sample is generally a bad idea, as the convergence of the
gradient descent is typically poor. 
As a solution we could use multiple workers to generate different \((s_i, a_i, s'_i, r_i)\), and update the 
parameter \(\phi\) by averaging the gradient. 
Alternatively we can use replay buffer, where we simply ignore step 1 in
Fitted Q Iteration. 
We just load some transition from a buffer, and we do not care where they come from. 
Ideally we should load data with diverse configuration of transition so that the whole configuration space
is covered.
Typically we do not just load a replay buffer with god given transitions, but rather we use a policy 
to collect some trajectories by interacting with the environment, load the buffer and 
train our algorithm. 
After some iterations, we use the latest policy with probability of deviation for exploring to collect new
trajectories.
We then have the <i>Q-Learning</i> algorithm:
</p>
<div class="card">
<div class="card-body bg-light mb-3">
<div class="card-header">Q-Learning with replay buffer</div>
  <ol>
    <li>Collect dataset \({(s_i, a_i, s'_i, r_i)}\) using some policy, add to buffer.</li>
    <li>Sample a batch \({(s_i, a_i, s'_i, r_i)}\) from the buffer. </li>
    <li>Set \(\phi \leftarrow \phi - \alpha \sum\limits_{i} \frac{dQ_{\phi}}{d\phi} \left(
        Q_{\phi}(s_i, a_i) - \left[ r(s_i, a_i) + \gamma \max\limits_{a'}Q_{\phi}(s'_i, a'_i)
        \right] \right)  \).</li>
    <li>Repeat step 2 and 3 for \(K\) times.</li>
    <li>Repeat step 1.</li>
  </ol>
</div>
</div>
<p>
Taking \(K=1\) and repeating the outer loop for every run will produce online version of 
Q iteration algorithm.
</p>
</div>

<div class="mb-5">
<h4>Moving Target</h4>
<p>
The problem with the Q-learning algorithm above is that we have two \(Q_{\phi} \) appear in the equation.
We want to optimize the parameter \(\phi\) but at the same time our target
\(r(s_i, a_i) + \gamma \max_{a'}Q_{\phi}(s'_i, a'_i)\) is also using the parameter \(\phi\).
We are essentially chasing our own tail. 
One solution is to fix the parameter \(\phi = \phi'\) for the target, and only update it after some
iterations. We update the Q-learning algorithm:
</p>
<div class="card">
<div class="card-body bg-light mb-3">
<div class="card-header">Q-Learning with replay buffer and target network</div>
  <ol>
    <li>Set target network parameters \(\phi' \leftarrow \phi\). </li>
    <li>Collect dataset \({(s_i, a_i, s'_i, r_i)}\) using some policy, add to buffer.</li>
    <li>Sample a batch \({(s_i, a_i, s'_i, r_i)}\) from the buffer. </li>
    <li>Set \(\phi \leftarrow \phi - \alpha \sum\limits_{i} \frac{dQ_{\phi}}{d\phi} \left(
        Q_{\phi}(s_i, a_i) - \left[ r(s_i, a_i) + \gamma \max \limits_{a'}Q_{\phi'}(s'_i, a'_i)
        \right] \right)  \).</li>
    <li>Repeat step 3 and 4 for \(K\) times.</li>
    <li>Repeat step 2 after \(N\) times.</li>
    <li>Repeat step 1 after \(M\) times.</li>
  </ol>
</div>
</div>
<p>
Setting \(K=1\) and \(N=1\) will yield the classic Deep Q-learning (DQN) algorithm. 
Note that the target network lags the current one by \(K\) times. 
One can use Polyak averaging during the update of \(\phi\):
\begin{align}
\phi' \leftarrow \beta \phi' + (1-\beta) \phi.
\end{align}

In this case, the target network has same lag as for each iteration. 
Whereas in the previous version of parameter update, the first iteration will use the newly updated parameter,
and after \(M-1\) step, we are lagging far behind the target network. 
</p>
</div>

<div class="mb-5">
<h4>Generalization</h4>
<p>
If you stare at the Q-Learning with replay buffer and target network for long, you can notice that several step
can be parallelized:
<ol>
<li>Process 1: Collect data with parallel workers with the current policy. Add the data to replay buffer.
    Remember to evict old data in FIFO style.</li>
<li>Process 2: Update the target network parameter \(\phi'\). Remember to boradcast it to all workers.</li>
<li>Process 3: Sample some data batches from replay buffer and perform the Q-function regression 
    to update the parameter \(\phi\).</li>
</ol>
For classic DQN, we have process 1 and 3 running at same speed, while process 2 occurs slowly.
For online Q-learning, all processes run at same speed. 
Having different processes to run at different rates can help Q-learning to converge effectively, 
as every problem has different characters in terms of data and parameter, i.e. how fast can data or training 
achieve stationarity. 
</p>
</div>
</div>

<script src="https://cdn.jsdelivr.net/npm/bootstrap@4.5.3/dist/js/bootstrap.bundle.min.js" integrity="sha384-ho+j7jyWK8fNQe+A12Hb8AhRq26LrZ/JpcUGGOn+Y7RsweNrtN/tE3MoK7ZeZDyx" crossorigin="anonymous"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/10.0.3/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad();</script>
</body>
</html>
